{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Unsupervised Learning Exercises\n",
    "***\n",
    "\n",
    "We will look at exercises to identify important features to get better regression results and also perform clustering with K-means clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hitters.csv').dropna().drop('Unnamed: 0', axis=1)\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: perform PCA on the feature vectors\n",
    "\n",
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_.T).loc[:4,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X_train))\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "n = len(X_reduced)\n",
    "kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "print('no PCA',mse)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, X.shape[-1]):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(np.arange(0,X.shape[-1],1),mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1)\n",
    "plt.xticks(np.arange(0,X.shape[-1], step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and clustering\n",
    "\n",
    "We are going to explore some dimensionality reduction techniques on the Iris dataset and visualize the low-dimensional representation. The goal is to provide a better understanding of what the dimensionality reduction techniques are doing.\n",
    "\n",
    "For more resources on data dimensionality reduction refer to:\n",
    "\n",
    "- [Really nice blogpost detailing dimension reduction and visualization with MNIST](https://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "- [The paper on TSNE, if you're interested in the math](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Iris dataset:\n",
    "Here we will perform Principal Components Analysis on the iris dataset using SKlearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell will take a few seconds\n",
    "\n",
    "X,y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, for easy visualization, we are going to work with a subset of the data.\n",
    "\n",
    "In the next cell, extract a subset of the data and perform PCA on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = X[:500,:], y[:500]\n",
    "\n",
    "#TO DO: perform PCA on the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize this data using the two principal dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "# TO DO: make a scatter plot of the data using the first two components of your \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize this data using the ***three*** principal dimensions. See, if it gets any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# TODO: create 3d scatter plot to visualize the projection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***K-means clustering*** on the Iris dataset using the reduced data using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: using scikit learn implement k means clustering. \n",
    "# Since this is Iris, we know that the number of outut categories is 3. Use that as number of clusters\n",
    "\n",
    "\n",
    "\n",
    "# TODO, next extract the labels of the output k-means. (These labels are just membership to different clusters)\n",
    "# Remember: K-means does not have any information about the actual labels.\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to visualize how the k-means algorithm has clustered the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a 2- D scatter plot to show this. You can use colors to represent the categorization by K-means. \n",
    "# Also consider annotating the points with the true labels to visually see how K-means has done.\n",
    "\n",
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a 3- D scatter plot to show this. You can use colors to represent the categorization by K-means. \n",
    "# Also consider annotating the points with the true labels to visually see how K-means has done.\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes(projection='3d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play around with the hyperparameters of K-means and visualize the output. See how the clustering changes with each.**\n",
    "\n",
    "- Initialization method (`init`)\n",
    "- Number of clusters (`n_clusters`)\n",
    "- Number of initializations  for k means (`n_init`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use this cell to visualize\n",
    "\n",
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow plot to determine number of clusters:\n",
    "\n",
    "For this example, we know exactly what the number of categories are. We therefore know how many clusters there should be. However, for truly unsupervised problems, we need to identify a good value for `k`. \n",
    "\n",
    "The elbow plot essentially plots how the sum of square distances of the data points to their closest cluster centers. You can determine this using the attribute `inertia_` in your k-means object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sq_distances = []\n",
    "\n",
    "# TODO: append values of the sum of squared distance \n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(range(1,20), sum_sq_distances, 'bX-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this compare with the clusters you chose to begin with?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction on MNIST for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell will take a few seconds\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, for easy visualization, we are going to work with a subset of the data.\n",
    "\n",
    "In the next cell, extract a subset of the data and perform PCA on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = X[:5000,:], y[:5000]\n",
    "\n",
    "# TODO: perform PCA on the MNIST digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data using the prinicpal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "# TO DO: make a scatter plot of the data using the first two components of your \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using t-SNE for visualizing MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use sklearn tsne to 'embed' the MNIST data into 2-dimensional space using stochastic neighbor embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatter plot of the embedding in the next cell. Color the points based on the true labels of the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(18, 16))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "# TO DO: make a scatter plot of the data using the first two components of your \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the t-sne visualization look compared to the PCA visualization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Play around with hyperparameters of the two to see how it changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
